\documentclass{beamer}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[english]{babel}
\usetheme[
    outer/progressbar=foot
]{metropolis}
\usepackage{listings}
\usepackage[scaled=.8]{beramono}
\usepackage{subcaption}
\usepackage{booktabs}

\title{Approximative Linear $t$-SNE Using $k$-Means}
\date{\today}
\author{Sonja Biedermann, BSc}
\institute{Fakultät für Informatik\\Universität Wien}
\titlegraphic{\hfill\includegraphics[height=1cm]{logo}}

\definecolor{asparagus}{rgb}{0.53, 0.66, 0.42}
\definecolor{alizarin}{rgb}{0.82, 0.1, 0.26}
\definecolor{chromeyellow}{rgb}{1.0, 0.77, 0.05}
\definecolor{chromeyellow}{rgb}{1.0, 0.65, 0.0}

\lstset{commentstyle=\color{darkgray},postbreak=\space,breakindent=5pt,breaklines,basicstyle=\ttfamily\footnotesize,label=DescriptiveLabel}

\makeatletter
\def\lst@MSkipToFirst{%
    \global\advance\lst@lineno\@ne
    \ifnum \lst@lineno=\lst@firstline
        \def\lst@next{\lst@LeaveMode \global\lst@newlines\z@
        \lst@OnceAtEOL \global\let\lst@OnceAtEOL\@empty
        \ifnum \c@lstnumber>0
            \vspace{2 mm}
        \fi
        \lst@InitLstNumber % Added to work with modified \lsthk@PreInit.
        \lsthk@InitVarsBOL
        \c@lstnumber=\numexpr-1+\lst@lineno % this enforces the displayed line numbers to always be the input line numbers
        \lst@BOLGobble}%
        \expandafter\lst@next
    \fi}
\makeatother

\definecolor{BlueTOL}{HTML}{0064a7}
\definecolor{BrownTOL}{HTML}{666633}
\definecolor{GreenTOL}{HTML}{225522}
% \setbeamercolor{normal text}{fg=BlueTOL,bg=white}

\definecolor{Purple}{HTML}{911146}
\definecolor{Orange}{HTML}{CF4A30}
\definecolor{Blue}{HTML}{0066a1}
\definecolor{DarkBlue}{HTML}{00466f}

\setbeamercolor{alerted text}{fg=Orange}
\setbeamercolor{frametitle}{bg=Purple}
\setbeamercolor{palette primary}{bg=Purple}

\begin{document}
\maketitle

\begin{frame}{Agenda}
    \setbeamertemplate{section in toc}[sections numbered]
    \setcounter{tocdepth}{1}
    \tableofcontents
\end{frame}

\section{Introduction}

\begin{frame}[fragile]{Introduction: Dimensionality Reduction}
    \begin{itemize}
        \item reducing the dimensionality of a data set while keeping interesting parts
            \begin{itemize}
                \item[~] e.g. variance, local neighborhoods
            \end{itemize}
        \item reduces computational cost, easier for other methods
        \item two general kinds:
            \begin{itemize}
                \item linear dimensionality reduction (LDR, e.g. PCA)
                \item non-linear dimensionality reduction (NLDR)
            \end{itemize}
        \item manifold learning (NLDR): data is already lying on a
            low-dimensional, potentially non-linear manifold
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Introduction: Dimensionality Reduction}
    \begin{figure}[h]
        \centering
        \includegraphics[width=.6\textwidth]{swissroll}
        \caption{The Swiss Roll data set}
    \end{figure}
\end{frame}

\section{$t$-SNE And Its Variants}

\subsection{Introduction}

\begin{frame}[fragile]{SNE}
    \begin{itemize}
        \item Stochastic Neighborhood Embedding: two probability distributions
            \begin{itemize}
                \item high dimensional: $p_{j | i}$, probability that $i$ picks $j$ as its neighbor
                \item low dimensional: $q_{j | i}$
            \end{itemize}
        \item optimization problem
        \item objective: Kullback-Leibler divergence
            \[
                KL(P \, || \, Q) = \sum_i \sum_j p_{ij} \log \frac{p_{ij}}{q_{ij}}
            \]
        \item find low-dimensional points whose neighborhood has similar encoding
        \item difficult to optimize: not convex, many local minima, costly
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{$t$-SNE}
    \begin{itemize}
        \item qualitative issue of SNE: crowding problem
            \begin{itemize}
                \item lower dimensional space has much less volume
                \item many points moderately far away, not enough space
            \end{itemize}
        \item solution: fat-tailed distribution in low-dimensional space
    \end{itemize}

    \begin{figure}[h]
        \centering
        \includegraphics[height=.5\textheight]{t_vs_norm}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{$t$-SNE}
    \begin{columns}
    \begin{column}{0.6\textwidth}
        \begin{itemize}
            \item popular due to visual properties
            \item problems:
                \begin{itemize}
                    \item difficult to optimize objective
                    \item computational complexity $\mathcal{O}(n^2)$
                    \item often misinterpreted
                \end{itemize}
            \item approaches:
                \begin{itemize}
                    \item approximate gradient
                    \item use different objective
                \end{itemize}
        \end{itemize}
    \end{column}
    \begin{column}{0.38\textwidth}
        \begin{figure}[h]
            \centering
            \includegraphics[width=\textwidth]{tsne_emb_cifar}
        \end{figure}
    \end{column}
\end{columns}
\end{frame}

\begin{frame}[fragile]{Related Work (Selection)}
    \begin{itemize}
        \item Barnes-Hut $t$-SNE
        \item FI$t$-SNE
        \item UMAP
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Related Work (Selection)}
    Barnes-Hut $t$-SNE
    \begin{itemize}
        \item use trees to
            \begin{itemize}
                \item speed up nearest neighbor search
                \item summarize points to speed up computation
            \end{itemize}
        \item computational complexity $\mathcal{O}(n \log n)$
        \item introduces quality-speed tradeoff parameter $\theta$
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Related Work (Selection)}
    FI$t$-SNE
    \begin{itemize}
        \item interpolates onto equispaced grid
        \item uses FFT to compute convolution
        \item uses random tree based library\footnote{\url{https://github.com/spotify/annoy}} to find ANNs
        \item computational complexity $\mathcal{O}(n)$
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Related Work (Selection)}
    UMAP
    \begin{itemize}
        \item another common criticism of $t$-SNE is the lack of theoretical foundation
        \item similar to $t$-SNE, but
            \begin{itemize}
                \item objective is cross entropy of two fuzzy sets
                \item different kernel in high-dimensional and low-dimensional space
                \item different normalization
            \end{itemize}
        \item objective allows usage of negative sampling
        \item empirical complexity of $\mathcal{O}(n^{1.14})$ \footnote{due to NNDescent, otherwise $\mathcal{O}(n)$}
    \end{itemize}
\end{frame}

\section{Approximative Linear $t$-SNE Using $k$-Means}

\begin{frame}[fragile]{Approximative Linear $t$-SNE}
    \begin{itemize}
        \item speed up nearest neighbor search
            \begin{itemize}
                \item Locality Sensitive Hashing
                \item widely used in similarity search
                \item intuition: maximize probability of similar items colliding
            \end{itemize}
        \item speed up computation of gradient
            \begin{itemize}
                \item summarize points by using $k$-Means
                \item clustering need not be good---run for 10 iterations
                \item cluster centers (centroids) summarize entire cluster
            \end{itemize}
    \end{itemize}
\end{frame}

\subsection{Using Approximate Nearest Neighbors}

\begin{frame}[fragile]{Using Approximative Nearest Neighbors: LSH}
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.7\textwidth]{srp}
        \caption{Hyperplane LSH}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Using $k$-Means}
    \begin{figure}[h]
        \centering
        \includegraphics[width=.6\textwidth]{voronoi}
        \caption{Voronoi diagram implied by $k$-Means clustering}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Putting it all together: $t$-SNE's gradient}
    \begin{align*}
        \frac{\partial KL(P \; || \; Q)}{\partial y_i} &= \sum_{j \neq i} p_{ij}q_{ij}Z(y_i - y_j) - \sum_{j \neq i} q_{ij}^2Z(y_i - y_j) \\
        &= F_\text{attr,$i$} + F_\text{rep,$i$}\\
    \end{align*}

    \vspace{-2em}
    % \quad where $Z = \sum_{k \neq l} (1 + || y_k - y_l||^2)^{-1}$ \\
    % \quad and $q_{ij} = \frac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_{k \neq l} (1 + ||y_k - y_k||^2)^{-1}}$

    \quad where
    \begin{align*}
        Z &= \sum_{k \neq l} (1 + || y_k - y_l||^2)^{-1} \\
        q_{ij} &= \frac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_{k \neq l} (1 + ||y_k - y_l||^2)^{-1}}
    \end{align*}

    \begin{itemize}
        \item speeding up $F_\text{attr}$ is simple
            \begin{itemize}
                \item \alert{Z cancels out denomiator of $q_{ij}$!}
                \item use only $k$ nearest neighbors $\rightarrow$ P becomes sparse
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Putting it all together: $t$-SNE's gradient}
    \begin{align*}
        F_\text{rep,$i$} &= \sum_{j \neq i} -q_{ij}^2Z(y_i - y_j)
    \end{align*}

    \begin{itemize}
        \item more difficult situation
        \item[$\rightarrow$] trick: approximate $F_\text{rep,$i$}Z = \sum_{j \neq i} -(q_{ij}Z)^2(y_i - y_j)$
            \bigskip
        \item summarize points by cluster centroids $\bar{y}_c, \; c = 1\ldots k$
        \item use $\bar{y}_c$ instead of $y_j$
        \item approximate $Z$ while computing $F_\text{rep,$i$}Z$
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Putting it all together: $t$-SNE's gradient}
    \quad Final result
    \begin{align*}
        \hat{F}_\text{rep,$i$}Z &= \sum_{c=1}^{k} - \frac{y_i - \bar{y}_c}{1 + ||y_i - \bar{y}_c||^2} \cdot n_c \\
        \hat{Z} &= \sum_{i = 1}^{n} \sum_{c=1}^{k} \frac{n_c}{1 + ||y_i - \bar{y}_c||^2}
    \end{align*}

    \quad where $n_c$ is the number of points in a cluster.
\end{frame}

\begin{frame}[fragile]{Putting it all together: Overview}
    \begin{figure}[h]
        \centering
        \includegraphics[width=\textwidth]{pseudo}
    \end{figure}
\end{frame}

\section{Experimental Results}

\begin{frame}[fragile]{Experimental Results: Nearest Neighborhood Purity}
    \begin{itemize}
        \item performed parameter tuning
            \begin{itemize}
                \item k $\rightarrow$ larger better, diminishing returns after $k = 30$
                \item perplexity and eta data dependent, used values in line with default parameters of other methods
                \item exaggeration: larger is better, but results in distortion
            \end{itemize}
        \item compared with 3 different methods (bhtsne, fitsne, umap) on 17 data sets
            of 1000 points up to more than one million
        \item performed PCA down to 50 dimensions in every case
        \item metric: nearest neighborhood purity with $k = 100$
            \begin{itemize}
                \item[~] do the labels of the nearest $k$ neighbors match?
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Experimental Results: Nearest Neighborhood Purity}
\fontsize{8pt}{7.9}\selectfont
\begin{table}[tb]
  \centering
  \begin{tabular}{lrrrr}
    \toprule
data & bhtsne & fitsne & umap & ktsne \\ \midrule
emailEuCore & 0.3127 & 0.3160 & \bfseries 0.3301 & 0.0722 \\
coil-20 & 0.5906 & 0.5641 & \bfseries 0.5920 & 0.4922 \\
cora & 0.6331 & 0.6283 & 0.6232 & \bfseries 0.6404 \\
citeseer & 0.4217 & 0.4262 & 0.4280 & \bfseries 0.4297 \\
optdigits & 0.9677 & 0.9656 & \bfseries 0.9736 & 0.9585 \\
youtube* & 0.7978 & 0.8011 & \bfseries 0.8124 & 0.7946 \\
dblp & 0.3393 & \bfseries 0.3401 & 0.3370 & 0.3100 \\
pubmed & \bfseries 0.7007 & 0.6985 & 0.6986 & 0.6948 \\
cifar & 0.8784 & 0.8773 & \bfseries 0.8801 & 0.8519 \\
mnist & 0.9536 & \bfseries 0.9539 & 0.9521 & 0.9380 \\
fashion\_mnist & 0.7417 & \bfseries 0.7426 & 0.7034 & 0.6889 \\
com-dblp* & 0.5303 & \bfseries 0.5907 & 0.5855 & 0.4580 \\
com-amazon* & 0.6982 & 0.7449 & \bfseries 0.7606 & 0.6047 \\
svhn & 0.1236 & 0.1238 & \bfseries 0.1240 & 0.1234 \\
hollywood* & 0.6695 & 0.7894 & \bfseries 0.8172 & 0.7844 \\
timit & 0.3125 & 0.3445 & \bfseries 0.3641 & 0.3481 \\
wiki-topcats* & 0.5060 & 0.7477 & \bfseries 0.8939 & 0.7968 \\
\bottomrule
  \end{tabular}
  \caption{Average nearest neighborhood purity, $k = 100$. Best in bold.}
\label{tab:comp}
\end{table}
\end{frame}

\begin{frame}[fragile]{Experimental Results: Runtime}
\fontsize{8pt}{7.9}\selectfont
\begin{table}[tb]
  \centering
  \begin{tabular}{lrrrr}
    \toprule
data & bhtsne & fitsne & umap & ktsne \\ \midrule
emailEuCore & 6.02 & 36.28 & 22.50 & \bfseries 4.54 \\
coil-20 & 9.74 & 52.67 & 24.16 & \bfseries 6.90 \\
cora & 27.26 & 52.10 & 34.54 & \bfseries 12.04 \\
citeseer & 44.52 & 56.02 & 39.49 & \bfseries 14.39 \\
optdigits & 68.22 & 52.12 & 57.48 & \bfseries 27.46 \\
youtube & 220.16 & \bfseries 50.52 & 101.55 & 66.70 \\
dblp & 402.66 & \bfseries 53.22 & 131.77 & 86.16 \\
pubmed & 478.61 & \bfseries 58.06 & 142.53 & 100.36 \\
cifar & 1507.18 & \bfseries 70.11 & 362.86 & 333.41 \\
mnist & 2284.13 & \bfseries 80.76 & 512.21 & 466.50 \\
fashion\_mnist & 2003.80 & \bfseries 80.63 & 534.86 & 515.43 \\
com-dblp & 25277.60 & \bfseries 345.12 & 2336.02 & 4517.64 \\
com-amazon & 27031.37 & \bfseries 360.54 & 2486.53 & 5450.24 \\
svhn & 25749.11 & \bfseries 656.79 & 6191.86 & 10212.89 \\
hollywood & 123778.42 & \bfseries 964.93 & 16137.21 & 11624.30 \\
timit & 190110.22 & \bfseries 1146.17 & 11170.68 & 13598.08 \\
wiki-topcats & 256839.20 & \bfseries 1421.12 & 17196.37 & 22242.01 \\
\bottomrule
  \end{tabular}
  \caption{Average runtime in seconds. Best in bold.}
\label{tab:comp}
\end{table}
\end{frame}

\begin{frame}[fragile]{Experimental Results: Runtime on synthetic data}
  \begin{figure}[h]
      \centering
      \includegraphics[width=0.9\textwidth]{runtime_comparison}
  \caption{Average runtime in seconds on synthetic data (log/log)}
  \end{figure}
\end{frame}

% \begin{frame}[fragile]{Some Embeddings}
    % \begin{figure}[h]
        % \vspace{-.5em}
        % \centering
        % \begin{subfigure}{0.3\textwidth}
            % \includegraphics[width=\textwidth]{emb/bhtsne_mnist}
            % \caption{bhtsne}
            % \vspace{-2em}
        % \end{subfigure}
        % \begin{subfigure}{0.3\textwidth}
            % \includegraphics[width=\textwidth]{emb/fitsne_mnist}
            % \caption{fitsne}
            % \vspace{-2em}
        % \end{subfigure}
        % \par\bigskip
        % \begin{subfigure}{0.3\textwidth}
            % \includegraphics[width=\textwidth]{emb/umap_mnist}
            % \caption{umap}
            % \vspace{-1em}
        % \end{subfigure}
        % \begin{subfigure}{0.3\textwidth}
            % \includegraphics[width=\textwidth]{emb/ktsne_mnist}
            % \caption{ktsne}
            % \vspace{-1em}
        % \end{subfigure}
        % \caption{MNIST}
    % \end{figure}
% \end{frame}

\begin{frame}[fragile]{Some Embeddings}
    \vspace{-.5em}
    \begin{figure}[h]
        \centering
        \begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{emb/bhtsne_fashion_mnist}
            \caption{bhtsne}
            \vspace{-2em}
        \end{subfigure}
        \begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{emb/fitsne_fashion_mnist}
            \caption{fitsne}
            \vspace{-2em}
        \end{subfigure}
        \par\bigskip
        \begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{emb/umap_fashion_mnist}
            \caption{umap}
            \vspace{-1em}
        \end{subfigure}
        \begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{emb/ktsne_fashion_mnist}
            \caption{ktsne}
            \vspace{-1em}
        \end{subfigure}
        \caption{Fashion MNIST}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Some Embeddings}
    \vspace{-.5em}
    \begin{figure}[h]
        \centering
        \begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{emb/bhtsne_com-amazon}
            \caption{bhtsne}
            \vspace{-2em}
        \end{subfigure}
        \begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{emb/fitsne_com-amazon}
            \caption{fitsne}
            \vspace{-2em}
        \end{subfigure}
        \par\bigskip
        \begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{emb/umap_com-amazon}
            \caption{umap}
            \vspace{-1em}
        \end{subfigure}
        \begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{emb/ktsne_com-amazon}
            \caption{ktsne}
            \vspace{-1em}
        \end{subfigure}
        \caption{com-amazon}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Some Embeddings}
    \vspace{-.5em}
    \begin{figure}[h]
        \centering
        \begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{emb/bhtsne_hollywood}
            \caption{bhtsne}
            \vspace{-2em}
        \end{subfigure}
        \begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{emb/fitsne_hollywood}
            \caption{fitsne}
            \vspace{-2em}
        \end{subfigure}
        \par\bigskip
        \begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{emb/umap_hollywood}
            \caption{umap}
            \vspace{-1em}
        \end{subfigure}
        \begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{emb/ktsne_hollywood}
            \caption{ktsne}
            \vspace{-1em}
        \end{subfigure}
        \caption{hollywood}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Some Embeddings}
    \vspace{-.5em}
    \begin{figure}[h]
        \centering
        \begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{emb/bhtsne_wiki-topcats}
            \caption{bhtsne}
            \vspace{-2em}
        \end{subfigure}
        \begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{emb/fitsne_wiki-topcats}
            \caption{fitsne}
            \vspace{-2em}
        \end{subfigure}
        \par\bigskip
        \begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{emb/umap_wiki-topcats}
            \caption{umap}
            \vspace{-1em}
        \end{subfigure}
        \begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{emb/ktsne_wiki-topcats}
            \caption{ktsne}
            \vspace{-1em}
        \end{subfigure}
        \caption{wiki-topcats}
    \end{figure}
\end{frame}

\section{Conclusion}

\begin{frame}{Conclusion}
    \begin{itemize}
        \item devised and implemented a variant of $t$-SNE
        \item evaluated its performance against three competing methods
        \item results were OK, but did not particularly excel
        \item future work: fine-grained parallelism in distance computation
    \end{itemize}
\end{frame}

\begin{frame}[standout]
    Thanks for your attention!
\end{frame}

\end{document}
