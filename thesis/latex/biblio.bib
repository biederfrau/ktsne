@article{eigenmaps,
author = {Belkin, Mikhail and Niyogi, Partha},
title = {Laplacian Eigenmaps for Dimensionality Reduction and Data Representation},
journal = {Neural Computation},
volume = {15},
number = {6},
pages = {1373-1396},
year = {2003},
doi = {10.1162/089976603321780317},

URL = {
        https://doi.org/10.1162/089976603321780317

},
eprint = {
        https://doi.org/10.1162/089976603321780317

}
,
    abstract = { One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed. }
},
@article{linearsurvey,
 author = {Cunningham, John P. and Ghahramani, Zoubin},
 title = {Linear Dimensionality Reduction: Survey, Insights, and Generalizations},
 journal = {J. Mach. Learn. Res.},
 issue_date = {January 2015},
 volume = {16},
 number = {1},
 month = jan,
 year = {2015},
 issn = {1532-4435},
 pages = {2859--2900},
 numpages = {42},
 url = {http://dl.acm.org/citation.cfm?id=2789272.2912091},
 acmid = {2912091},
 publisher = {JMLR.org},
 keywords = {dimensionality reduction, eigenvector problems, matrix manifolds},
},

@article{tsne,
  added-at = {2015-06-19T12:07:15.000+0200},
  author = {van der Maaten, Laurens and Hinton, Geoffrey},
  biburl = {https://www.bibsonomy.org/bibtex/28b9aebb404ad4a4c6a436ea413550b30/lopusz_kdd},
  interhash = {370ba8b9e1909b61880a6f47c93bcd49},
  intrahash = {8b9aebb404ad4a4c6a436ea413550b30},
  journal = {Journal of Machine Learning Research},
  keywords = {dimensionality_reduction tSNE visualization},
  pages = {2579--2605},
  timestamp = {2015-08-19T15:19:11.000+0200},
  title = {Visualizing Data using {t-SNE} },
  url = {http://www.jmlr.org/papers/v9/vandermaaten08a.html},
  volume = 9,
  year = 2008
},

@article{bhtsne,
  author  = {Laurens van der Maaten},
  title   = {Accelerating t-SNE using Tree-Based Algorithms},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  pages   = {3221-3245},
  url     = {http://jmlr.org/papers/v15/vandermaaten14a.html}
},

@conference{sne_tsne_survey,
author={Rodolphe Priam.},
title={Symmetric Generative Methods and tSNE: A Short Survey},
booktitle={Proceedings of the 13th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications - Volume 3: IVAPP,},
year={2018},
pages={356-363},
publisher={SciTePress},
organization={INSTICC},
doi={10.5220/0006684303560363},
isbn={978-989-758-289-9},
},








@article{pearson_pca,
author = { Karl   Pearson },
title = {LIII. On lines and planes of closest fit to systems of points in space},
journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
volume = {2},
number = {11},
pages = {559-572},
year  = {1901},
publisher = {Taylor & Francis},
doi = {10.1080/14786440109462720},

URL = {
        https://doi.org/10.1080/14786440109462720

},
eprint = {
        https://doi.org/10.1080/14786440109462720

}

},

    @article{algo_nmf,
author = {Lee, Daniel and Seung, Hyunjune},
year = {2001},
month = {02},
pages = {},
title = {Algorithms for Non-negative Matrix Factorization},
volume = {13},
journal = {Adv. Neural Inform. Process. Syst.}
},

    @article{lda_tut,
author = {Tharwat, Alaa and Gaber, Tarek and Ibrahim, Abdelhameed and Hassanien, Aboul Ella},
year = {2017},
month = {05},
pages = {169-190,},
title = {Linear discriminant analysis: A detailed tutorial},
volume = {30},
journal = {Ai Communications},
doi = {10.3233/AIC-170729}
}




,
@article{robust_pca_galpin,
 author = {Galpin, Jacqueline S. and Hawkins, Douglas M.},
 title = {Methods of L1-estimation of a Covariance Matrix},
 journal = {Comput. Stat. Data Anal.},
 issue_date = {Sept. 1987},
 volume = {5},
 number = {4},
 month = sep,
 year = {1987},
 issn = {0167-9473},
 pages = {305--319},
 numpages = {15},
 url = {http://dx.doi.org/10.1016/0167-9473(87)90054-5},
 doi = {10.1016/0167-9473(87)90054-5},
 acmid = {36123},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
},

@article{robust_pca_gao,
 author = {Gao, Junbin},
 title = {Robust L1 Principal Component Analysis and Its Bayesian Variational Inference},
 journal = {Neural Comput.},
 issue_date = {February 2008},
 volume = {20},
 number = {2},
 month = feb,
 year = {2008},
 issn = {0899-7667},
 pages = {555--572},
 numpages = {18},
 url = {http://dx.doi.org/10.1162/neco.2007.11-06-397},
 doi = {10.1162/neco.2007.11-06-397},
 acmid = {1331162},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
},

@article{r1_pca,
author = {Ding, Chris and Zhou, Ding and He, Xiaofeng and Zha, Hongyuan},
year = {2006},
month = {01},
pages = {281-288},
title = {R 1-PCA: Rotational invariant L 1-norm principal component analysis for robust subspace factorization},
volume = {2006},
journal = {ICML 2006 - Proceedings of the 23rd International Conference on Machine Learning},
doi = {10.1145/1143844.1143880}
},

@article{pca_pp,
title = "Algorithms for Projection–Pursuit robust principal component analysis",
journal = "Chemometrics and Intelligent Laboratory Systems",
volume = "87",
number = "2",
pages = "218 - 225",
year = "2007",
issn = "0169-7439",
doi = "https://doi.org/10.1016/j.chemolab.2007.01.004",
url = "http://www.sciencedirect.com/science/article/pii/S016974390700007X",
author = "C. Croux and P. Filzmoser and M.R. Oliveira",
keywords = "Multivariate statistics, Optimization, Numerical precision, Outliers, Robustness, Scale estimators",
abstract = "The results of a standard principal component analysis (PCA) can be affected by the presence of outliers. Hence robust alternatives to PCA are needed. One of the most appealing robust methods for principal component analysis uses the Projection–Pursuit principle. Here, one projects the data on a lower-dimensional space such that a robust measure of variance of the projected data will be maximized. The Projection–Pursuit-based method for principal component analysis has recently been introduced in the field of chemometrics, where the number of variables is typically large. In this paper, it is shown that the currently available algorithm for robust Projection–Pursuit PCA performs poor in the presence of many variables. A new algorithm is proposed that is more suitable for the analysis of chemical data. Its performance is studied by means of simulation experiments and illustrated on some real data sets."
},

@book{bishop,
 author = {Bishop, Christopher M.},
 title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
 year = {2006},
 isbn = {0387310738},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
},

@article{robust_pca_candes,
 author = {Cand\`{e}s, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
 title = {Robust Principal Component Analysis?},
 journal = {J. ACM},
 issue_date = {May 2011},
 volume = {58},
 number = {3},
 month = jun,
 year = {2011},
 issn = {0004-5411},
 pages = {11:1--11:37},
 articleno = {11},
 numpages = {37},
 url = {http://doi.acm.org/10.1145/1970392.1970395},
 doi = {10.1145/1970392.1970395},
 acmid = {1970395},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {\ℓ1-norm minimization, Principal components, duality, low-rank matrices, nuclear-norm minimization, robustness vis-a-vis outliers, sparsity, video surveillance},
},

    @article{robust_pca_pp_li_chen,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2288497},
 abstract = {This article proposes and discusses a type of new robust estimators for covariance/correlation matrices and principal components via projection-pursuit techniques. The most attractive advantage of the new procedures is that they are of both rotational equivariance and high breakdown point. Besides, they are qualitatively robust and consistent at elliptic underlying distributions. The Monte Carlo study shows that the best of the new estimators compare favorably with other robust methods. They provide as good a performance as M-estimators and somewhat better empirical breakdown properties.},
 author = {Guoying Li and Zhonglian Chen},
 journal = {Journal of the American Statistical Association},
 number = {391},
 pages = {759--766},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Projection-Pursuit Approach to Robust Dispersion Matrices and Principal Components: Primary Theory and Monte Carlo},
 volume = {80},
 year = {1985}
}

,

    @Article{probabilistic_pca,
author = {Tipping, M. E. and Bishop, Christopher},
title = {Probabilistic Principal Component Analysis},
year = {1999},
month = {January},
abstract = {Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based upon a probability model. In this paper we demonstrate how the principal axes of a set of observed data vectors may be determined through maximum-likelihood estimation of parameters in a latent variable model closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss, with illustrative examples, the advantages conveyed by this probabilistic approach to PCA.},
url = {https://www.microsoft.com/en-us/research/publication/probabilistic-principal-component-analysis/},
pages = {611-622},
journal = {Journal of the Royal Statistical Society, Series B},
volume = {21},
number = {3},
note = {Available from  http://www.ncrg.aston.ac.uk/Papers/index.html},
},

@book{fukunaga,
 author = {Fukunaga, Keinosuke},
 title = {Introduction to Statistical Pattern Recognition (2Nd Ed.)},
 year = {1990},
 isbn = {0-12-269851-7},
 publisher = {Academic Press Professional, Inc.},
 address = {San Diego, CA, USA},
},

    @Article{torgerson,
author="Torgerson, Warren S.",
title="Multidimensional scaling: I. Theory and method",
journal="Psychometrika",
year="1952",
month="Dec",
day="01",
volume="17",
number="4",
pages="401--419",
abstract="Multidimensional scaling can be considered as involving three basic steps. In the first step, a scale of comparative distances between all pairs of stimuli is obtained. This scale is analogous to the scale of stimuli obtained in the traditional paired comparisons methods. In this scale, however, instead of locating each stimulus-object on a given continuum, the distances between each pair of stimuli are located on a distance continuum. As in paired comparisons, the procedures for obtaining a scale of comparative distances leave the true zero point undetermined. Hence, a comparative distance is not a distance in the usual sense of the term, but is a distance minus an unknown constant. The second step involves estimating this unknown constant. When the unknown constant is obtained, the comparative distances can be converted into absolute distances. In the third step, the dimensionality of the psychological space necessary to account for these absolute distances is determined, and the projections of stimuli on axes of this space are obtained. A set of analytical procedures was developed for each of the three steps given above, including a least-squares solution for obtaining comparative distances by the complete method of triads, two practical methods for estimating the additive constant, and an extension of Young and Householder's Euclidean model to include procedures for obtaining the projections of stimuli on axes from fallible absolute distances.",
issn="1860-0980",
doi="10.1007/BF02288916",
url="https://doi.org/10.1007/BF02288916"
}
,

    @Inbook{cox_mds,
author="Cox, Michael A. A.
and Cox, Trevor F.",
title="Multidimensional Scaling",
bookTitle="Handbook of Data Visualization",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="315--347",
abstract="Suppose dissimilarity data have been collected on a set of n objects or individuals, where there is a value of dissimilarity measured for each pair.The dissimilarity measure used might be a subjective judgement made by a judge, where for example a teacher subjectively scores the strength of friendship between pairs of pupils in her class, or, as an alternative, more objective, measure, she might count the number of contacts made in a day between each pair of pupils. In other situations the dissimilarity measure might be based on a data matrix. The general aim of multidimensional scaling is to find a configuration of points in a space, usually Euclidean, where each point represents one of the objects or individuals, and the distances between pairs of points in the configuration match as well as possible the original dissimilarities between the pairs of objects or individuals. Such configurations can be found using metric and non-metric scaling, which are covered in Sects. 2 and 3. A number of other techniques are covered by the umbrella title of multidimensional scaling (MDS), and here the techniques of Procrustes analysis, unidimensional scaling, individual differences scaling, correspondence analysis and reciprocal averaging are briefly introduced and illustrated with pertinent data sets.",
isbn="978-3-540-33037-0",
doi="10.1007/978-3-540-33037-0_14",
url="https://doi.org/10.1007/978-3-540-33037-0_14"
},

    @article{borg_groenen_mds,
author = {Borg, Ingwer and Groenen, Patrick},
year = {2006},
month = {06},
pages = {277 - 280},
title = {Modern Multidimensional Scaling: Theory and Applications},
volume = {40},
journal = {Journal of Educational Measurement},
doi = {10.1111/j.1745-3984.2003.tb01108.x}
},

@Article{kruskal_mds,
author="Kruskal, J. B.",
title="Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis",
journal="Psychometrika",
year="1964",
month="Mar",
day="01",
volume="29",
number="1",
pages="1--27",
abstract="Multidimensional scaling is the problem of representingn objects geometrically byn points, so that the interpoint distances correspond in some sense to experimental dissimilarities between objects. In just what sense distances and dissimilarities should correspond has been left rather vague in most approaches, thus leaving these approaches logically incomplete. Our fundamental hypothesis is that dissimilarities and distances are monotonically related. We define a quantitative, intuitively satisfying measure of goodness of fit to this hypothesis. Our technique of multidimensional scaling is to compute that configuration of points which optimizes the goodness of fit. A practical computer program for doing the calculations is described in a companion paper.",
issn="1860-0980",
doi="10.1007/BF02289565",
url="https://doi.org/10.1007/BF02289565"
}

,

    @article{vandermaaten_review,
  abstract = {In recent years, a variety of nonlinear dimensionality reduction techniques have been
proposed that aim to address the limitations of traditional techniques such as PCA
and classical scaling. The paper presents a review and systematic comparison of
these techniques. The performances of the nonlinear techniques are investigated on
artificial and natural tasks. The results of the experiments reveal that nonlinear tech-
niques perform well on selected artificial tasks, but that this strong performance does
not necessarily extend to real-world tasks. The paper explains these results by identi-
fying weaknesses of current nonlinear techniques, and suggests how the performance
of nonlinear dimensionality reduction techniques may be improved.

Real-world data, such as speech signals, digital photographs, or fMRI scans, usually has a high dimen-
sionality. In order to handle such real-world data adequately, its dimensionality needs to be reduced.
Dimensionality reduction is the transformation of high-dimensional data into a meaningful representa-
tion of reduced dimensionality. Ideally, the reduced representation should have a dimensionality that
corresponds to the intrinsic dimensionality of the data. The intrinsic dimensionality of data is the mini-
mum number of parameters needed to account for the observed properties of the data [49]. Dimension-
ality reduction is important in many domains, since it mitigates the curse of dimensionality and other
undesired properties of high-dimensional spaces [69]. As a result, dimensionality reduction facilitates,
among others, classification, visualization, and compression of high-dimensional data. Traditionally, di-
mensionality reduction was performed using linear techniques such as Principal Components Analysis
(PCA) [98], factor analysis [117], and classical scaling [126]. However, these linear techniques cannot
adequately handle complex nonlinear data.
In the last decade, a large number of nonlinear techniques for dimensionality reduction have been
proposed. See for an overview, e.g., [26, 110, 83, 131]. In contrast to the traditional linear techniques,
the nonlinear techniques have the ability to deal with complex nonlinear data. In particular for real-
world data, the nonlinear dimensionality reduction techniques may offer an advantage, because real-
world data is likely to form a highly nonlinear manifold. Previous studies have shown that nonlinear
techniques outperform their linear counterparts on complex artificial tasks. For instance, the Swiss roll
dataset comprises a set of points that lie on a spiral-like two-dimensional manifold that is embedded
within a three-dimensional space. A vast number of nonlinear techniques are perfectly able to find this
embedding, whereas linear techniques fail to do so. In contrast to these successes on artificial datasets,
successful applications of nonlinear dimensionality reduction techniques on natural datasets are less
convincing. Beyond this observation, it is not clear to what extent the performances of the various
dimensionality reduction techniques differ on artificial and natural tasks (a comparison is performed in
[94], but this comparison is very limited in scope with respect to the number of techniques and tasks
that are addressed).
Motivated by the lack of a systematic comparison of dimensionality reduction techniques, this paper
presents a comparative study of the most important linear dimensionality reduction technique (PCA),
and twelve frontranked nonlinear dimensionality reduction techniques. The aims of the paper are (1)
to investigate to what extent novel nonlinear dimensionality reduction techniques outperform the tradi-
tional PCA on real-world datasets and (2) to identify the inherent weaknesses of the twelve nonlinear
dimensionality reduction techniques. The investigation is performed by both a theoretical and an empir-
ical evaluation of the dimensionality reduction techniques. The identification is performed by a careful
analysis of the empirical results on specifically designed artificial datasets and on a selection of real-
world datasets.
Next to PCA, the paper investigates the following twelve nonlinear techniques: (1) Kernel PCA,
(2) Isomap, (3) Maximum Variance Unfolding, (4) diffusion maps, (5) Locally Linear Embedding,
(6) Laplacian Eigenmaps, (7) Hessian LLE, (8) Local Tangent Space Analysis, (9) Sammon mapping,
(10) multilayer autoencoders, (11) Locally Linear Coordination, and (12) manifold charting.},
  added-at = {2016-04-14T01:21:52.000+0200},
  author = {Van Der Maaten, Laurens and Postma, Eric and Van den Herik, Jaap},
  biburl = {https://www.bibsonomy.org/bibtex/2ed03568f0e9bca9cdaf6b25304e55940/peter.ralph},
  interhash = {f1c39ec766293d0203a327a6dd5d9948},
  intrahash = {ed03568f0e9bca9cdaf6b25304e55940},
  journal = {J Mach Learn Res},
  keywords = {PCA data_analysis machine_learning methods review visualization},
  pages = {66-71},
  timestamp = {2016-04-14T01:23:22.000+0200},
  title = {Dimensionality reduction: a comparative review},
  volume = 10,
  year = 2009
}

,
@article {isomap,
	author = {Tenenbaum, Joshua B. and Silva, Vin de and Langford, John C.},
	title = {A Global Geometric Framework for Nonlinear Dimensionality Reduction},
	volume = {290},
	number = {5500},
	pages = {2319--2323},
	year = {2000},
	doi = {10.1126/science.290.5500.2319},
	publisher = {American Association for the Advancement of Science},
	abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs{\textemdash}30,000 auditory nerve fibers or 106 optic nerve fibers{\textemdash}a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/290/5500/2319},
	eprint = {https://science.sciencemag.org/content/290/5500/2319.full.pdf},
	journal = {Science}
},



@article{pca_tut,
  author    = {Jonathon Shlens},
  title     = {A Tutorial on Principal Component Analysis},
  journal   = {CoRR},
  volume    = {abs/1404.1100},
  year      = {2014},
  url       = {http://arxiv.org/abs/1404.1100},
  archivePrefix = {arXiv},
  eprint    = {1404.1100},
  timestamp = {Mon, 13 Aug 2018 16:46:55 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Shlens14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

,

    @article{essential_loops,
title = "Nonlinear dimensionality reduction of data manifolds with essential loops",
journal = "Neurocomputing",
volume = "67",
pages = "29 - 53",
year = "2005",
note = "Geometrical Methods in Neural Networks and Learning",
issn = "0925-2312",
doi = "https://doi.org/10.1016/j.neucom.2004.11.042",
url = "http://www.sciencedirect.com/science/article/pii/S0925231205001104",
author = "John Aldo Lee and Michel Verleysen",
keywords = "Non-linear dimensionality reduction, Geodesic distance, Graph distance, Spanning tree, Graph cut",
abstract = "Numerous methods or algorithms have been designed to solve the problem of nonlinear dimensionality reduction (NLDR). However, very few among them are able to embed efficiently ‘circular’ manifolds like cylinders or tori, which have one or more essential loops. This paper presents a simple and fast procedure that can tear or cut those manifolds, i.e. break their essential loops, in order to make their embedding in a low-dimensional space easier. The key idea is the following: starting from the available data points, the tearing procedure represents the underlying manifold by a graph and then builds a maximum subgraph with no loops anymore. Because it works with a graph, the procedure can preprocess data for all NLDR techniques that uses the same representation. Recent techniques using geodesic distances (Isomap, geodesic Sammon's mapping, geodesic CCA, etc.) or K-ary neighborhoods (LLE, hLLE, Laplacian eigenmaps) fall in that category. After describing the tearing procedure in details, the paper comments a few experimental results."
},

@article {isomap_stability,
	author = {Balasubramanian, Mukund and Schwartz, Eric L.},
	title = {The Isomap Algorithm and Topological Stability},
	volume = {295},
	number = {5552},
	pages = {7--7},
	year = {2002},
	doi = {10.1126/science.295.5552.7a},
	publisher = {American Association for the Advancement of Science},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/295/5552/7},
	eprint = {https://science.sciencemag.org/content/295/5552/7.full.pdf},
	journal = {Science}
},

    @book{kernel_methods,
 author = {Shawe-Taylor, John and Cristianini, Nello},
 title = {Kernel Methods for Pattern Analysis},
 year = {2004},
 isbn = {0521813972},
 publisher = {Cambridge University Press},
 address = {New York, NY, USA},
}

,
    @inproceedings{intro_mvu,
 author = {Weinberger, Killan Q. and Saul, Lawrence K.},
 title = {An Introduction to Nonlinear Dimensionality Reduction by Maximum Variance Unfolding},
 booktitle = {Proceedings of the 21st National Conference on Artificial Intelligence - Volume 2},
 series = {AAAI'06},
 year = {2006},
 isbn = {978-1-57735-281-5},
 location = {Boston, Massachusetts},
 pages = {1683--1686},
 numpages = {4},
 url = {http://dl.acm.org/citation.cfm?id=1597348.1597471},
 acmid = {1597471},
 publisher = {AAAI Press},
} ,

@inproceedings{mvu,
 author = {Weinberger, Kilian Q. and Saul, Lawrence K.},
 title = {Unsupervised Learning of Image Manifolds by Semidefinite Programming},
 booktitle = {Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
 series = {CVPR'04},
 year = {2004},
 location = {Washington, D.C., USA},
 pages = {988--995},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=1896300.1896444},
 acmid = {1896444},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}
,

@article{rmvu,
author = {Hou, Chenping and Jiao, Yuanyuan and Wu, Yi and Yi, Dongyun},
year = {2008},
month = {07},
pages = {},
title = {Relaxed maximum-variance unfolding},
volume = {47},
journal = {Optical Engineering - OPT ENG},
doi = {10.1117/1.2956373}
},

    @article{intro_lle,
author = {Saul, Lawrence and Roweis, Sam},
year = {2001},
month = {01},
pages = {},
title = {An introduction to locally linear embedding},
volume = {7},
journal = {Journal of Machine Learning Research}
},

    @article {lle,
	author = {Roweis, Sam T. and Saul, Lawrence K.},
	title = {Nonlinear Dimensionality Reduction by Locally Linear Embedding},
	volume = {290},
	number = {5500},
	pages = {2323--2326},
	year = {2000},
	doi = {10.1126/science.290.5500.2323},
	publisher = {American Association for the Advancement of Science},
	abstract = {Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/290/5500/2323},
	eprint = {https://science.sciencemag.org/content/290/5500/2323.full.pdf},
	journal = {Science}
},

@inproceedings{planar_arrangement,
author = {Lim, Ik Soo and de Heras Ciechomski, Pablo and Sarni, Sofiane and Thalmann, Daniel},
year = {2003},
month = {07},
pages = {50- 55},
title = {Planar arrangement of high-dimensional biomedical data sets by Isomap coordinates},
isbn = {0-7695-1901-6},
journal = {Proceedings of the IEEE Symposium on Computer-Based Medical Systems},
doi = {10.1109/CBMS.2003.1212766}
},

    @inproceedings{motion_data,
author = {Jenkins, Odest and Mataric, Maja},
year = {2002},
month = {02},
pages = {2551 - 2556 vol.3},
title = {Deriving action and behavior primitives from human motion data},
volume = {3},
isbn = {0-7803-7398-7},
journal = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IRDS.2002.1041654}
},







@article{leigs,
author = {Belkin, Mikhail and Niyogi, Partha},
title = {Laplacian Eigenmaps for Dimensionality Reduction and Data Representation},
journal = {Neural Computation},
volume = {15},
number = {6},
pages = {1373-1396},
year = {2003},
doi = {10.1162/089976603321780317},

URL = {
        https://doi.org/10.1162/089976603321780317

},
eprint = {
        https://doi.org/10.1162/089976603321780317

}
,
    abstract = { One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed. }
}

,



@article{largevis,
  author    = {Jian Tang and
               Jingzhou Liu and
               Ming Zhang and
               Qiaozhu Mei},
  title     = {Visualization Large-scale and High-dimensional Data},
  journal   = {CoRR},
  volume    = {abs/1602.00370},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.00370},
  archivePrefix = {arXiv},
  eprint    = {1602.00370},
  timestamp = {Mon, 13 Aug 2018 16:45:58 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/TangLZM16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

,

@inproceedings{rand_proj_trees,
 author = {Dasgupta, Sanjoy and Freund, Yoav},
 title = {Random Projection Trees and Low Dimensional Manifolds},
 booktitle = {Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing},
 series = {STOC '08},
 year = {2008},
 isbn = {978-1-60558-047-0},
 location = {Victoria, British Columbia, Canada},
 pages = {537--546},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1374376.1374452},
 doi = {10.1145/1374376.1374452},
 acmid = {1374452},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {curse of dimension, k-d tree, manifold, random projection},
} ,

    @incollection{rand_proj_trees_rev,
title = {Random Projection Trees Revisited},
author = {Aman Dhesi and Kar, Purushottam},
booktitle = {Advances in Neural Information Processing Systems 23},
editor = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta},
pages = {496--504},
year = {2010},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4039-random-projection-trees-revisited.pdf}
},

@inproceedings{nn_descent,
 author = {Dong, Wei and Moses, Charikar and Li, Kai},
 title = {Efficient K-nearest Neighbor Graph Construction for Generic Similarity Measures},
 booktitle = {Proceedings of the 20th International Conference on World Wide Web},
 series = {WWW '11},
 year = {2011},
 isbn = {978-1-4503-0632-4},
 location = {Hyderabad, India},
 pages = {577--586},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1963405.1963487},
 doi = {10.1145/1963405.1963487},
 acmid = {1963487},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {arbitrary similarity measure, iterative method, k-nearest neighbor graph},
},

@article{kd_tree,
 author = {Bentley, Jon Louis},
 title = {Multidimensional Binary Search Trees Used for Associative Searching},
 journal = {Commun. ACM},
 issue_date = {Sept. 1975},
 volume = {18},
 number = {9},
 month = sep,
 year = {1975},
 issn = {0001-0782},
 pages = {509--517},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/361002.361007},
 doi = {10.1145/361002.361007},
 acmid = {361007},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {associative retrieval, attribute, binary search trees, binary tree insertion, information retrieval system, intersection queries, key, nearest neighbor queries, partial match queries},
}

,

    @inproceedings{kd_tree_rev,
 author = {Ram, Parikshit and Sinha, Kaushik},
 title = {Revisiting Kd-tree for Nearest Neighbor Search},
 booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
 series = {KDD '19},
 year = {2019},
 isbn = {978-1-4503-6201-6},
 location = {Anchorage, AK, USA},
 pages = {1378--1388},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/3292500.3330875},
 doi = {10.1145/3292500.3330875},
 acmid = {3330875},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {nearest-neighbor search, randomized algorithms, similarity search, space-partitioning trees},
},

    @article{balltree,
 author = {Liu, Ting and Moore, Andrew W. and Gray, Alexander},
 title = {New Algorithms for Efficient High-Dimensional Nonparametric Classification},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2006},
 volume = {7},
 month = dec,
 year = {2006},
 issn = {1532-4435},
 pages = {1135--1158},
 numpages = {24},
 url = {http://dl.acm.org/citation.cfm?id=1248547.1248588},
 acmid = {1248588},
 publisher = {JMLR.org},
},

@Article{umap_bio,
author={Becht, Etienne
and McInnes, Leland
and Healy, John
and Dutertre, Charles-Antoine
and Kwok, Immanuel W. H.
and Ng, Lai Guan
and Ginhoux, Florent
and Newell, Evan W.},
title={Dimensionality reduction for visualizing single-cell data using UMAP},
journal={Nature Biotechnology},
year={2019},
volume={37},
number={1},
pages={38-44},
abstract={A benchmarking analysis on single-cell RNA-seq and mass cytometry data reveals the best-performing technique for dimensionality reduction.},
issn={1546-1696},
doi={10.1038/nbt.4314},
url={https://doi.org/10.1038/nbt.4314}
}

,

@ARTICLE{umap,
       author = {{McInnes}, Leland and {Healy}, John and {Melville}, James},
        title = "{UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Computational Geometry, Computer Science - Machine Learning},
         year = "2018",
        month = "Feb",
          eid = {arXiv:1802.03426},
        pages = {arXiv:1802.03426},
archivePrefix = {arXiv},
       eprint = {1802.03426},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180203426M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

,

@article{kernel_pca_metric_mds,
 author = {Williams, Christopher K. I.},
 title = {On a Connection Between Kernel PCA and Metric Multidimensional Scaling},
 journal = {Mach. Learn.},
 issue_date = {2002},
 volume = {46},
 number = {1-3},
 month = mar,
 year = {2002},
 issn = {0885-6125},
 pages = {11--19},
 numpages = {9},
 url = {https://doi.org/10.1023/A:1012485807823},
 doi = {10.1023/A:1012485807823},
 acmid = {599658},
 publisher = {Kluwer Academic Publishers},
 address = {Norwell, MA, USA},
 keywords = {MDS, eigenproblem, kernel PCA, metric multidimensional scaling},
},

@article {art_tsne,
	author = {Kobak, Dmitry and Berens, Philipp},
	title = {The art of using t-SNE for single-cell transcriptomics},
	elocation-id = {453449},
	year = {2019},
	doi = {10.1101/453449},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Single-cell transcriptomics yields ever growing data sets containing RNA expression levels for thousands of genes from up to millions of cells. Common data analysis pipelines include a dimensionality reduction step for visualising the data in two dimensions, most frequently performed using t-distributed stochastic neighbour embedding (t-SNE). It excels at revealing local structure in high-dimensional data, but naive applications often suffer from severe shortcomings, e.g. the global structure of the data is not represented accurately. Here we describe how to circumvent such pitfalls, and develop a protocol for creating more faithful t-SNE visualisations. It includes PCA initialisation, a high learning rate, and multi-scale similarity kernels; for very large data sets, we additionally use exaggeration and downsampling-based initialisation. We use published single-cell RNA-seq data sets to demonstrate that this protocol yields superior results compared to the naive application of t-SNE.},
	URL = {https://www.biorxiv.org/content/early/2019/05/20/453449},
	eprint = {https://www.biorxiv.org/content/early/2019/05/20/453449.full.pdf},
	journal = {bioRxiv}
},

    @article{automatic_tsne,
  author    = {Yanshuai Cao and
               Luyu Wang},
  title     = {Automatic Selection of t-SNE Perplexity},
  journal   = {CoRR},
  volume    = {abs/1708.03229},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.03229},
  archivePrefix = {arXiv},
  eprint    = {1708.03229},
  timestamp = {Mon, 13 Aug 2018 16:47:03 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-03229},
  bibsource = {dblp computer science bibliography, https://dblp.org}
},

    @Article{bh,
author={Barnes, Josh
and Hut, Piet},
title={A hierarchical O(N log N) force-calculation algorithm},
journal={Nature},
year={1986},
volume={324},
number={6096},
pages={446-449},
abstract={Until recently the gravitational N-body problem has been modelled numerically either by direct integration, in which the computation needed increases as N2, or by an iterative potential method in which the number of operations grows as N log N. Here we describe a novel method of directly calculating the force on N bodies that grows only as N log N. The technique uses a tree-structured hierarchical subdivision of space into cubic cells, each of which is recursively divided into eight subcells whenever more than one particle is found to occupy the same cell. This tree is constructed anew at every time step, avoiding ambiguity and tangling. Advantages over potential-solving codes are: accurate local interactions; freedom from geometrical assumptions and restrictions; and applicability to a wide class of systems, including (proto-)planetary, stellar, galactic and cosmological ones. Advantages over previous hierarchical tree-codes include simplicity and the possibility of rigorous analysis of error. Although we concentrate here on stellar dynamical applications, our techniques of efficiently handling a large number of long-range interactions and concentrating computational effort where most needed have potential applications in other areas of astrophysics as well.},
issn={1476-4687},
doi={10.1038/324446a0},
url={https://doi.org/10.1038/324446a0}
}

,

    @article{fitsne,
  author    = {George C. Linderman and
               Manas Rachh and
               Jeremy G. Hoskins and
               Stefan Steinerberger and
               Yuval Kluger},
  title     = {Efficient Algorithms for t-distributed Stochastic Neighborhood Embedding},
  journal   = {CoRR},
  volume    = {abs/1712.09005},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.09005},
  archivePrefix = {arXiv},
  eprint    = {1712.09005},
  timestamp = {Mon, 13 Aug 2018 16:46:04 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1712-09005},
  bibsource = {dblp computer science bibliography, https://dblp.org}
},

    @article{misread_tsne,
  author = {Wattenberg, Martin and Viégas, Fernanda and Johnson, Ian},
  title = {How to Use t-SNE Effectively},
  journal = {Distill},
  year = {2016},
  url = {http://distill.pub/2016/misread-tsne},
  doi = {10.23915/distill.00002}
},
    @article{hlle,
author = {Donoho, David and Grimes, Carrie},
year = {2003},
month = {06},
pages = {5591-6},
title = {Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data. Proc. National Academy of Science (PNAS), 100, 5591-5596},
volume = {100},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
doi = {10.1073/pnas.1031596100}
},

    @incollection{mod_lle,
title = {MLLE: Modified Locally Linear Embedding Using Multiple Weights},
author = {Zhenyue Zhang and Jing Wang},
booktitle = {Advances in Neural Information Processing Systems 19},
editor = {B. Sch\"{o}lkopf and J. C. Platt and T. Hoffman},
pages = {1593--1600},
year = {2007},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/3132-mlle-modified-locally-linear-embedding-using-multiple-weights.pdf}
},



@article{ltsa,
  author    = {Zhenyue Zhang and
               Hongyuan Zha},
  title     = {Principal Manifolds and Nonlinear Dimension Reduction via Local Tangent
               Space Alignment},
  journal   = {CoRR},
  volume    = {cs.LG/0212008},
  year      = {2002},
  url       = {http://arxiv.org/abs/cs.LG/0212008},
  timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/cs-LG-0212008},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

,

    @article{sammon_mapping,
 author = {Sammon, J. W.},
 title = {A Nonlinear Mapping for Data Structure Analysis},
 journal = {IEEE Trans. Comput.},
 issue_date = {May 1969},
 volume = {18},
 number = {5},
 month = may,
 year = {1969},
 issn = {0018-9340},
 pages = {401--409},
 numpages = {9},
 url = {https://doi.org/10.1109/T-C.1969.222678},
 doi = {10.1109/T-C.1969.222678},
 acmid = {1310727},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Clustering, dimensionality reduction, mappings, multidimensional scaling, multivariate data analysis, nonparametric, pattern recognition, statistics.},
},

    @article{graph_embedding,
  author    = {HongYun Cai and
               Vincent W. Zheng and
               Kevin Chen{-}Chuan Chang},
  title     = {A Comprehensive Survey of Graph Embedding: Problems, Techniques and
               Applications},
  journal   = {CoRR},
  volume    = {abs/1709.07604},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.07604},
  archivePrefix = {arXiv},
  eprint    = {1709.07604},
  timestamp = {Mon, 13 Aug 2018 16:47:51 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1709-07604},
  bibsource = {dblp computer science bibliography, https://dblp.org}
},

    @inproceedings{lsh,
 author = {Gionis, Aristides and Indyk, Piotr and Motwani, Rajeev},
 title = {Similarity Search in High Dimensions via Hashing},
 booktitle = {Proceedings of the 25th International Conference on Very Large Data Bases},
 series = {VLDB '99},
 year = {1999},
 isbn = {1-55860-615-7},
 pages = {518--529},
 numpages = {12},
 url = {http://dl.acm.org/citation.cfm?id=645925.671516},
 acmid = {671516},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
},

    @inproceedings{kmeans,
  title={Some methods for classification and analysis of multivariate observations},
  author={MacQueen, James and others},
  booktitle={Proceedings of the fifth Berkeley symposium on mathematical statistics and probability},
  volume={1},
  number={14},
  pages={281--297},
  year={1967},
  organization={Oakland, CA, USA}
},

   @article {phate,
	author = {Moon, Kevin R. and van Dijk, David and Wang, Zheng and Chen, William and Hirn, Matthew J. and Coifman, Ronald R. and Ivanova, Natalia B. and Wolf, Guy and Krishnaswamy, Smita},
	title = {PHATE: A Dimensionality Reduction Method for Visualizing Trajectory Structures in High-Dimensional Biological Data},
	elocation-id = {120378},
	year = {2017},
	doi = {10.1101/120378},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {In recent years, dimensionality reduction methods have become critical for visualization, exploration, and interpretation of high-throughput, high-dimensional biological data, as they enable the extraction of major trends in the data while discarding noise. However, biological data contains a type of predominant structure that is not preserved in commonly used methods such as PCA and tSNE, namely, branching progression structure. This structure, which is often non-linear, arises from underlying biological processes such as differentiation, graded responses to stimuli, and population drift, which generate cellular (or population) diversity. We propose a novel, affinity-preserving embedding called PHATE (Potential of Heat-diffusion for Affinity-based Trajectory Embedding), designed explicitly to preserve progression structure in data.PHATE provides a denoised, two or three-dimensional visualization of the complete branching trajectory structure in high-dimensional data. It uses heat-diffusion processes, which naturally denoise the data, to compute cell-cell affinities. Then, PHATE creates a diffusion-potential geometry by free-energy potentials of these processes. This geometry captures high-dimensional trajectory structures, while enabling a natural embedding of the intrinsic data geometry. This embedding accurately visualizes trajectories and data distances, without requiring strict assumptions typically used by path-finding and tree-fitting algorithms, which have recently been used for pseudotime orderings or tree-renderings of cellular data. Furthermore, PHATE supports a wide range of data exploration tasks by providing interpretable overlays on top of the visualization. We show that such overlays can emphasize and reveal trajectory end-points, branch points and associated split-decisions, progression-forming variables (e.g., specific genes), and paths between developmental events in cellular state-space. We demonstrate PHATE on single-cell RNA sequencing and mass cytometry data pertaining to embryoid body differentiation, IPSC reprogramming, and hematopoiesis in the bone marrow. We also demonstrate PHATE on non-single cell data including single-nucleotide polymorphism (SNP) measurements of European populations, and 16s sequencing of gut microbiota.},
	URL = {https://www.biorxiv.org/content/early/2017/03/24/120378},
	eprint = {https://www.biorxiv.org/content/early/2017/03/24/120378.full.pdf},
	journal = {bioRxiv}
}
,

    @article {umap_rebuttal,
	author = {Kobak, Dmitry and Linderman, George C.},
	title = {UMAP does not preserve global structure any better than t-SNE when using the same initialization},
	elocation-id = {2019.12.19.877522},
	year = {2019},
	doi = {10.1101/2019.12.19.877522},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {One of the most ubiquitous analysis tools employed in single-cell transcriptomics and cytometry is t-distributed stochastic neighbor embedding (t-SNE) [1], used to visualize individual cells as points on a 2D scatter plot such that similar cells are positioned close together. Recently, a related algorithm, called uniform manifold approximation and projection (UMAP) [2] has attracted substantial attention in the single-cell community. In Nature Biotechnology, Becht et al. [3] argued that UMAP is preferable to t-SNE because it better preserves the global structure of the data and is more consistent across runs. Here we show that this alleged superiority of UMAP can be entirely attributed to different choices of initialization in the implementations used by Becht et al.: t-SNE implementations by default used random initialization, while the UMAP implementation used a technique called Laplacian eigenmaps [4] to initialize the embedding. We show that UMAP with random initialization preserves global structure as poorly as t-SNE with random initialization, while t-SNE with informative initialization performs as well as UMAP with informative initialization. Hence, contrary to the claims of Becht et al., their experiments do not demonstrate any advantage of the UMAP algorithm per se, but rather warn against using random initialization.},
	URL = {https://www.biorxiv.org/content/early/2019/12/19/2019.12.19.877522},
	eprint = {https://www.biorxiv.org/content/early/2019/12/19/2019.12.19.877522.full.pdf},
	journal = {bioRxiv}
},

@book{mmds,
  added-at = {2014-04-02T11:11:28.000+0200},
  author = {Rajaraman, Anand and Leskovec, Jure and Ullman, Jeffrey D.},
  biburl = {https://www.bibsonomy.org/bibtex/23ef788252184bf10f6906a6563924794/jaeschke},
  interhash = {33042928a3c4cc2cb7da4b7d5c27f863},
  intrahash = {3ef788252184bf10f6906a6563924794},
  keywords = {bigdata book data dataset mining},
  timestamp = {2014-07-28T15:57:31.000+0200},
  title = {Mining Massive Datasets},
  url = {http://infolab.stanford.edu/~ullman/mmds/book.pdf},
  year = 2014
},

@inproceedings{pstablelsh,
author = {Datar, Mayur and Immorlica, Nicole and Indyk, Piotr and Mirrokni, Vahab S.},
title = {Locality-Sensitive Hashing Scheme Based on p-Stable Distributions},
year = {2004},
isbn = {1581138857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/997817.997857},
doi = {10.1145/997817.997857},
abstract = {We present a novel Locality-Sensitive Hashing scheme for the Approximate Nearest Neighbor Problem under lp norm, based on p-stable distributions.Our scheme improves the running time of the earlier algorithm for the case of the lp norm. It also yields the first known provably efficient approximate NN algorithm for the case p<1. We also show that the algorithm finds the exact near neigbhor in O(log n) time for data satisfying certain "bounded growth" condition.Unlike earlier schemes, our LSH scheme works directly on points in the Euclidean space without embeddings. Consequently, the resulting query time bound is free of large factors and is simple and easy to implement. Our experiments (on synthetic data sets) show that the our data structure is up to 40 times faster than kd-tree.},
booktitle = {Proceedings of the Twentieth Annual Symposium on Computational Geometry},
pages = {253–262},
numpages = {10},
keywords = {p-stable distributions, approximate nearest neighbor, locally sensitive hashing, sublinear algorithm},
location = {Brooklyn, New York, USA},
series = {SCG '04}
},

@article{lshcomp,
  abstract = {It is well known that high-dimensional nearest neighbor retrieval is very expensive. Dramatic performance gains are obtained using approximate search schemes, such as the popular Locality-Sensitive Hashing (LSH). Several extensions have been proposed to address the limitations of this algorithm, in particular, by choosing more appropriate hash functions to better partition the vector space. All the proposed extensions, however, rely on a structured quantizer for hashing, poorly fitting real data sets, limiting its performance in practice. In this paper, we compare several families of space hashing functions in a real setup, namely when searching for high-dimension SIFT descriptors. The comparison of random projections, lattice quantizers, k-means and hierarchical k-means reveal that unstructured quantizer significantly improves the accuracy of LSH, as it closely fits the data in the feature space. We then compare two querying mechanisms introduced in the literature with the one originally proposed in LSH, and discuss their respective merits and limitations.},
  added-at = {2011-07-14T08:09:23.000+0200},
  author = {Paulevé, Loïc and Jégou, Hervé and Amsaleg, Laurent},
  biburl = {https://www.bibsonomy.org/bibtex/2af70f777f2f9c7e94b81d0a66e5bb81d/stroeh},
  description = {Pattern Recognition Letters : Locality sensitive hashing: A comparison of hash function types and querying mechanisms},
  doi = {10.1016/j.patrec.2010.04.004},
  interhash = {0b1d7a8ed1874d0ed5de4bd455b32a7a},
  intrahash = {af70f777f2f9c7e94b81d0a66e5bb81d},
  issn = {0167-8655},
  journal = {Pattern Recognition Letters},
  keywords = {duplicate hashing nearest neighbor similarity},
  number = 11,
  pages = {1348 - 1358},
  timestamp = {2011-07-14T08:09:24.000+0200},
  title = {Locality sensitive hashing: A comparison of hash function types and querying mechanisms},
  url = {http://hal.inria.fr/inria-00567191/en/},
  volume = 31,
  year = 2010
},

    @inproceedings{multiprobelsh,
author = {Lv, Qin and Josephson, William and Wang, Zhe and Charikar, Moses and Li, Kai},
year = {2007},
month = {01},
pages = {950-961},
title = {Multi-Probe LSH: Efficient Indexing for High-Dimensional Similarity Search .},
journal = {VLDB}
},

@inproceedings{simsearchhighdim,
  author    = {Roger Weber and
               Hans{-}J{\"{o}}rg Schek and
               Stephen Blott},
  editor    = {Ashish Gupta and
               Oded Shmueli and
               Jennifer Widom},
  title     = {A Quantitative Analysis and Performance Study for Similarity-Search
               Methods in High-Dimensional Spaces},
  booktitle = {VLDB'98, Proceedings of 24rd International Conference on Very Large
               Data Bases, August 24-27, 1998, New York City, New York, {USA}},
  pages     = {194--205},
  publisher = {Morgan Kaufmann},
  year      = {1998},
  url       = {http://www.vldb.org/conf/1998/p194.pdf},
  timestamp = {Thu, 12 Mar 2020 11:33:39 +0100},
  biburl    = {https://dblp.org/rec/conf/vldb/WeberSB98.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
},

    @inproceedings{falconn,
author = {Andoni, Alexandr and Indyk, Piotr and Laarhoven, Thijs and Razenshteyn, Ilya and Schmidt, Ludwig},
title = {Practical and Optimal LSH for Angular Distance},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH [1, 2]), our algorithm is also practical, improving upon the well-studied hyperplane LSH [3] in practice. We also introduce a multiprobe version of this algorithm and conduct an experimental evaluation on real and synthetic data sets.We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1225–1233},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
},

    @inproceedings{sphericallsh2,
author = {Andoni, Alexandr and Razenshteyn, Ilya},
title = {Optimal Data-Dependent Hashing for Approximate Near Neighbors},
year = {2015},
isbn = {9781450335362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2746539.2746553},
doi = {10.1145/2746539.2746553},
abstract = {We show an optimal data-dependent hashing scheme for the approximate near neighbor problem. For an n-point dataset in a d-dimensional space our data structure achieves query time O(d ⋅ nρ+o(1)) and space O(n1+ρ+o(1) + d ⋅ n), where ρ=1/(2c2-1) for the Euclidean space and approximation c&gt;1. For the Hamming space, we obtain an exponent of ρ=1/(2c-1). Our result completes the direction set forth in (Andoni, Indyk, Nguyen, Razenshteyn 2014) who gave a proof-of-concept that data-dependent hashing can outperform classic Locality Sensitive Hashing (LSH). In contrast to (Andoni, Indyk, Nguyen, Razenshteyn 2014), the new bound is not only optimal, but in fact improves over the best (optimal) LSH data structures (Indyk, Motwani 1998) (Andoni, Indyk 2006) for all approximation factors c&gt;1.From the technical perspective, we proceed by decomposing an arbitrary dataset into several subsets that are, in a certain sense, pseudo-random.},
booktitle = {Proceedings of the Forty-Seventh Annual ACM Symposium on Theory of Computing},
pages = {793–801},
numpages = {9},
keywords = {decision trees, high-dimensional geometry, similarity search, theory, data structures},
location = {Portland, Oregon, USA},
series = {STOC '15}
}

@article{verse,
  author    = {Anton Tsitsulin and
               Davide Mottin and
               Panagiotis Karras and
               Emmanuel M{\"{u}}ller},
  title     = {{VERSE:} Versatile Graph Embeddings from Similarity Measures},
  journal   = {CoRR},
  volume    = {abs/1803.04742},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.04742},
  archivePrefix = {arXiv},
  eprint    = {1803.04742},
  timestamp = {Mon, 13 Aug 2018 16:49:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-04742.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
},

@article{sne,
  added-at = {2012-01-16T13:25:40.000+0100},
  author = {Hinton, Geoffrey and Roweis, Sam},
  biburl = {https://www.bibsonomy.org/bibtex/2a0d72c90aa3348858a647e7603ad7323/gromgull},
  description = {Stochastic Neighbor Embedding | Mendeley},
  editor = {S Becker, S Thrun and Obermayer, KEditors},
  interhash = {e29fa9b96e5445390b32830bc42e69ca},
  intrahash = {a0d72c90aa3348858a647e7603ad7323},
  journal = {Advances in neural information processing systems},
  keywords = {dimensionality-reduction embedding machine-learning visualisation},
  pages = {833--840},
  publisher = {Citeseer},
  timestamp = {2012-01-16T13:25:40.000+0100},
  title = {Stochastic Neighbor Embedding},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.13.7959&rep=rep1&type=pdf},
  volume = 15,
  year = 2003
},

    @article{arttsne,
  title={The art of using t-SNE for single-cell transcriptomics},
  author={D. Kobak and Philipp Berens},
  journal={Nature Communications},
  year={2019},
  volume={10}
}

@misc{annoy,
  title = {{ANNOY} library},
  howpublished = {\url{https://github.com/spotify/annoy}},
  note = {Accessed: 2021-03-11}
}

@misc{pca_parametric,
  title = {{P}rojection of {N}ew {D}ata in {K}ernel {PCA}},
  howpublished = {https://amber0309.github.io/2016/04/14/kpca/},
  note = {Accessed: 2021-04-09}
},

@article{elki,
  author    = {Erich Schubert and
               Arthur Zimek},
  title     = {{ELKI:} {A} large open-source library for data analysis - {ELKI} Release
               0.7.5 "Heidelberg"},
  journal   = {CoRR},
  volume    = {abs/1902.03616},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.03616},
  archivePrefix = {arXiv},
  eprint    = {1902.03616},
  timestamp = {Sat, 23 Jan 2021 01:11:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-03616.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
},

@TECHREPORT{coil,
    author = {Sameer A. Nene and Shree K. Nayar and Hiroshi Murase},
    title = {Columbia Object Image Library (COIL-20},
    institution = {},
    year = {1996}
},

@misc{snapnets,
  author       = {Jure Leskovec and Andrej Krevl},
  title        = {{SNAP Datasets}: {Stanford} Large Network Dataset Collection},
  howpublished = {\url{http://snap.stanford.edu/data}},
  month        = jun,
  year         = 2014
},



    @inproceedings{nr,
         title={The Network Data Repository with Interactive Graph Analytics and Visualization},
         author={Ryan A. Rossi and Nesreen K. Ahmed},
         booktitle={AAAI},
         url={http://networkrepository.com},
         year={2015}
    },

@misc{uci,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }


@misc{socialcomputingasu,
  title = {{ASU} {S}ocialcomputing {Y}ou{T}ube data set},
  howpublished = {http://socialcomputing.asu.edu/datasets/YouTube},
  note = {Accessed: 2019-01-28, unfortunately now offline}
},

@article{lecun-mnist,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
},

@TECHREPORT{tinyimages,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {},
    year = {2009}
},

@article{famnist,
  author    = {Han Xiao and
               Kashif Rasul and
               Roland Vollgraf},
  title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning
               Algorithms},
  journal   = {CoRR},
  volume    = {abs/1708.07747},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.07747},
  archivePrefix = {arXiv},
  eprint    = {1708.07747},
  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1708-07747.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
},

@article{svhn,
author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew},
year = {2011},
month = {01},
pages = {},
title = {Reading Digits in Natural Images with Unsupervised Feature Learning},
journal = {NIPS}
},

@data{timit,
author = {Garofolo, John and Lamel, Lori and Fisher, William and Fiscus, Jonathan and Pallett, David and Dahlgren, Nancy and Zue, Victor},
publisher = {Abacus Data Network},
title = {{TIMIT Acoustic-Phonetic Continuous Speech Corpus}},
year = {1993},
version = {V1},
doi = {11272.1/AB2/SWVENO},
url = {https://hdl.handle.net/11272.1/AB2/SWVENO}
},

 @MISC{eigenweb,
  author = {Ga\"{e}l Guennebaud and Beno\^{i}t Jacob and others},
  title = {Eigen v3},
  howpublished = {http://eigen.tuxfamily.org},
  year = {2010}
 },

 @article{vieclus,
author = {Biedermann, Sonja and Henzinger, Monika and Schulz, Christian and Schuster, Bernhard},
year = {2018},
month = {02},
pages = {},
title = {Memetic Graph Clustering}
},

    @inproceedings{gempe,
author = {Plant, Claudia and Biedermann, Sonja and B\"{o}hm, Christian},
title = {Data Compression as a Comprehensive Framework for Graph Drawing and Representation Learning},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403174},
doi = {10.1145/3394486.3403174},
abstract = {Embedding a graph into feature space is a promising approach to understand its structure. Embedding into 2D or 3D space enables visualization; representation in higher-dimensional vector space (typically &gt;100D) enables the application of data mining techniques. For the success of knowledge discovery it is essential that the distances between the embedded vertices truly reflect the structure of the graph. Our fundamental idea is to compress the adjacency matrix by predicting the existence of an edge from the Euclidean distance between the corresponding vertices in the embedding, and to use the achieved compression as a quality measure for the embedding. We call this quality measure Predictive Entropy (PE). PE uses a sigmoid function to define the probability which is monotonically decreasing with the Euclidean distance. We use this sigmoid probability to compress the adjacency matrix of the graph by an entropy coding. While PE could be used to assess the result of any graph drawing or representation learning method we particularly use it as objective function in our new method GEMPE (Graph Embedding by Minimizing the Predictive Entropy). We demonstrate in our experiments that GEMPE clearly outperforms comparison methods with respect to quality of the visual result, clustering and node-labeling accuracy on the discovered coordinates.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {1212–1222},
numpages = {11},
keywords = {graph embedding, minimum description length},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}
